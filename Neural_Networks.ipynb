{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Preprocessing Data For Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.12541308,  1.96429418],\n",
       "       [-1.15329466, -0.50068741],\n",
       "       [ 0.29529406, -0.22809346],\n",
       "       [ 0.57385917, -0.42335076],\n",
       "       [ 1.40955451, -0.81216255]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=np.array([[-100.1,3240.1],\n",
    "                  [-200.2,-234.1],[5000.5,150.1],[6000.6,-125.1],[9000.9,-673.1]])\n",
    "scaler=preprocessing.StandardScaler()\n",
    "features_standardized=scaler.fit_transform(features)\n",
    "features_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "network=models.Sequential()\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(10,)))\n",
    "network.add(layers.Dense(units=16,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\"))\n",
    "network.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "250/250 [==============================] - 9s 12ms/step - loss: 0.5091 - accuracy: 0.7570 - val_loss: 0.3513 - val_accuracy: 0.8485\n",
      "Epoch 2/3\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.3192 - accuracy: 0.8680 - val_loss: 0.3247 - val_accuracy: 0.8614\n",
      "Epoch 3/3\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.3021 - accuracy: 0.8778 - val_loss: 0.3320 - val_accuracy: 0.8586\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# set a Random Seed()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# set the number of features we want\n",
    "number_of_features=1000\n",
    "\n",
    "#load data and targetvector from movie review data\n",
    "\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "#Do one hot encoding method for features matrix\n",
    "\n",
    "tokenizer=Tokenizer(num_words=number_of_features)\n",
    "features_train=tokenizer.sequences_to_matrix(data_train,mode=\"binary\")\n",
    "features_test=tokenizer.sequences_to_matrix(data_test,mode=\"binary\")\n",
    "\n",
    "#Build neural network model\n",
    "\n",
    "network=models.Sequential(0)\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\"))\n",
    "network.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])\n",
    "\n",
    "#train neural Network Model\n",
    "\n",
    "history=network.fit(features_train,target_train,epochs=3,verbose=1,batch_size=100,validation_data=(features_test,target_test))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training a Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_of_features=5000\n",
    "\n",
    "data=reuters.load_data(num_words=number_of_features)\n",
    "(data_train,target_vector_train),(data_test,target_vector_test)=data\n",
    "\n",
    "#one-hot encoding\n",
    "\n",
    "tokenizer=Tokenizer(num_words=number_of_features)\n",
    "features_train=tokenizer.sequences_to_matrix(data_train,mode=\"binary\")\n",
    "features_test=tokenizer.sequences_to_matrix(data_test,mode=\"binary\")\n",
    "\n",
    "target_train=to_categorical(target_vector_train)\n",
    "target_test=to_categorical(target_vector_test)\n",
    "\n",
    "#start neural network model\n",
    "\n",
    "network=models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=100,activation=\"relu\",input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=100,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=46,activation=\"softmax\"))\n",
    "network.compile(loss=\"categorical_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])\n",
    "\n",
    "#train neural network\n",
    "\n",
    "history=network.fit(features_train,\n",
    "                   target_train,\n",
    "                   epochs=3,\n",
    "                   verbose=0,\n",
    "                   batch_size=100,\n",
    "                   validation_data=(features_test,target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "67/67 [==============================] - 1s 7ms/step - loss: 17240.9064 - mse: 17240.9064 - val_loss: 17730.3652 - val_mse: 17730.3652\n",
      "Epoch 2/10\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 17034.2706 - mse: 17034.2690 - val_loss: 16615.5508 - val_mse: 16615.5508\n",
      "Epoch 3/10\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 15913.0741 - mse: 15913.0741 - val_loss: 14689.2529 - val_mse: 14689.2529\n",
      "Epoch 4/10\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 13620.3028 - mse: 13620.3028 - val_loss: 11952.5527 - val_mse: 11952.5527\n",
      "Epoch 5/10\n",
      "67/67 [==============================] - 0s 4ms/step - loss: 10563.6785 - mse: 10563.6782 - val_loss: 8606.3457 - val_mse: 8606.3457\n",
      "Epoch 6/10\n",
      "67/67 [==============================] - 0s 4ms/step - loss: 7613.5298 - mse: 7613.5298 - val_loss: 5126.1411 - val_mse: 5126.1411\n",
      "Epoch 7/10\n",
      "67/67 [==============================] - 0s 4ms/step - loss: 4032.5618 - mse: 4032.5618 - val_loss: 2282.0310 - val_mse: 2282.0310\n",
      "Epoch 8/10\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 1714.2412 - mse: 1714.2412 - val_loss: 812.7551 - val_mse: 812.7551\n",
      "Epoch 9/10\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 639.1804 - mse: 639.1804 - val_loss: 356.8598 - val_mse: 356.8598\n",
      "Epoch 10/10\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 314.8223 - mse: 314.8223 - val_loss: 227.3920 - val_mse: 227.3920\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#set random seed\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "#generate features matrix and target vectors\n",
    "\n",
    "features,target=make_regression(n_samples=10000,n_features=3,n_informative=3,\n",
    "                               n_targets=1,\n",
    "                               noise=0.0,\n",
    "                               random_state=0)\n",
    "# divide the data into training and testing set\n",
    "\n",
    "features_train,features_test,target_train,target_test=train_test_split(features,target,test_size=0.33,random_state=0)\n",
    "\n",
    "#start neural network\n",
    "\n",
    "network=models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=32,activation=\"relu\",input_shape=(features_train.shape[1],)))\n",
    "network.add(layers.Dense(units=32,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1))\n",
    "\n",
    "#compile neural network\n",
    "\n",
    "network.compile(loss=\"mse\", #mean squared error\n",
    "               optimizer=\"RMSprop\",#optimization algorithm\n",
    "               metrics=[\"mse\"])#mean squared error\n",
    "#train Neural Network\n",
    "\n",
    "history=network.fit(features_train,\n",
    "                   target_train,\n",
    "                   epochs=10,\n",
    "                   verbose=1,\n",
    "                   batch_size=100,\n",
    "                   validation_data=(features_test,target_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4a9c9840fbf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#start building a neural network model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\User\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m   1155\u001b[0m                \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m                **kwargs):\n\u001b[1;32m-> 1157\u001b[1;33m     super(Dense, self).__init__(\n\u001b[0m\u001b[0;32m   1158\u001b[0m         activity_regularizer=activity_regularizer, **kwargs)\n\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\User\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\User\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m         \u001b[0mbatch_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_shape'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "#prediction using feed forward neural nework\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "#set number of features we want\n",
    "\n",
    "number_of_features=10000\n",
    "\n",
    "#load data and target vector from imdb\n",
    "\n",
    "data=imdb.load_data(num_words=number_of_features)\n",
    "(data_train,target_train),(data_test,target_test)=data\n",
    "\n",
    "#one-hot encoding format\n",
    "\n",
    "tokenizer=Tokenizer(num_words=number_of_features)\n",
    "features_train=tokenizer.sequences_to_matrix(data_train,mode=\"binary\")\n",
    "features_test=tokenizer.sequences_to_matrix(data_test,mode=\"binary\")\n",
    "\n",
    "#start building a neural network model\n",
    "network=models.Sequential()\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(number_of_features)))\n",
    "network.add(layers.Dense(units=16,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\"))\n",
    "\n",
    "#compile the network\n",
    "network.compile(loss=\"binary_crossentropy\",optimizer=\"RMSprop\",\n",
    "               metrics=[\"accuracy\"])\n",
    "#train neural network\n",
    "history=network.fit(features_train,\n",
    "                   target_train,\n",
    "                   epochs=3,\n",
    "                   verbose=1,batch_size=100,\n",
    "                   validation_data=(features_test,target_test))\n",
    "#predict class test\n",
    "predicted_target=network.predict(features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid keyword argument(s) in `compile`: {'los'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-dff3d8513a66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m#compile a neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m network.compile(los=\"bianary_crossentropy\",\n\u001b[0m\u001b[0;32m     36\u001b[0m                \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rmsprop\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                metrics=[\"accuracy\"])\n",
      "\u001b[1;32mF:\\User\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m           \u001b[0msteps_per_execution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'experimental_steps_per_execution'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_eagerly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\User\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_validate_compile\u001b[1;34m(self, optimizer, metrics, **kwargs)\u001b[0m\n\u001b[0;32m   2517\u001b[0m     \u001b[0minvalid_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'sample_weight_mode'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2518\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minvalid_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2519\u001b[1;33m       raise TypeError('Invalid keyword argument(s) in `compile`: %s' %\n\u001b[0m\u001b[0;32m   2520\u001b[0m                       (invalid_kwargs,))\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid keyword argument(s) in `compile`: {'los'}"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "#set the number of features you want\n",
    "\n",
    "number_of_features=10000\n",
    "\n",
    "#load data and review data\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "#one-hot encoding \n",
    "tokenizer=Tokenizer(num_words=number_of_features)\n",
    "features_train=tokenizer.sequences_to_matrix(data_train,mode=\"binary\")\n",
    "features_test=tokenizer.sequences_to_matrix(data_test,mode=\"binary\")\n",
    "\n",
    "#start neural Model\n",
    "network=models.Sequential()\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16,activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1,activation=\"sigmoid\"))\n",
    "\n",
    "#compile a neural network\n",
    "network.compile(los=\"binary_crossentropy\",\n",
    "               optimizer=\"RMSprop\",\n",
    "               metrics=[\"accuracy\"])\n",
    "#train model\n",
    "history=network.fit(features_train,\n",
    "                   target_train,\n",
    "                   epochs=15,\n",
    "                   verbose=0,\n",
    "                   validation_data=(features_test,target_test))\n",
    "#get training and test loss history\n",
    "training_loss=history.history(\"loss\")\n",
    "test_loss=history.history(\"val_loss\")\n",
    "\n",
    "#create count number of epochs\n",
    "epoch_count=range(1,len(training_loss)+1)\n",
    "\n",
    "#visualize loss history\n",
    "\n",
    "plt.plot(epoch_count,training_loss,\"r--\")\n",
    "plt.plot(epoch_count,test_loss,\"b-\")\n",
    "plt.legend([\"Training Loss\",\"Test_loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Overfitting with weight Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
